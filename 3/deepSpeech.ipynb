{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wav_files = glob.glob('VCTK-Corpus/**/**/*.wav')\n",
    "txt_files = [file.replace('data', 'txt').replace('.wav', '.txt') for file in wav_files]\n",
    "train_data, test_data = train_test_split(list(zip(wav_files[:50], txt_files[:50])))\n",
    "\n",
    "with open('data/train.csv', 'w') as file_train:\n",
    "    for wav_file, txt_file in train_data:\n",
    "        file_train.write(wav_file + ',' + txt_file + '\\n')\n",
    "with open('data/test.csv', 'w') as file_test:\n",
    "    for wav_file, txt_file in test_data:\n",
    "        file_test.write(wav_file + ',' + txt_file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Save directory already exists.\n",
      "DeepSpeech(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(0, 10))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "    (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "  )\n",
      "  (rnns): Sequential(\n",
      "    (0): BatchRNN(\n",
      "      (rnn): GRU(672, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (1): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (2): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (3): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (4): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): SequenceWise (\n",
      "    Sequential(\n",
      "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): Linear(in_features=800, out_features=29, bias=False)\n",
      "    ))\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax(\n",
      "  )\n",
      ")\n",
      "Number of parameters: 38067968\n",
      "Epoch: [1][1/19]\tTime 180.827 (180.827)\tData 0.224 (0.224)\tLoss 2386.2781 (2386.2781)\t\n",
      "Epoch: [1][2/19]\tTime 57.689 (119.258)\tData 0.001 (0.112)\tLoss 923.8889 (1655.0835)\t\n",
      "Epoch: [1][3/19]\tTime 133.969 (124.162)\tData 0.000 (0.075)\tLoss 704.7341 (1338.3004)\t\n",
      "Epoch: [1][4/19]\tTime 102.645 (118.782)\tData 0.000 (0.056)\tLoss 454.9078 (1117.4522)\t\n",
      "Epoch: [1][5/19]\tTime 105.177 (116.061)\tData 0.001 (0.045)\tLoss 87.3080 (911.4234)\t\n",
      "Epoch: [1][6/19]\tTime 88.029 (111.389)\tData 0.001 (0.038)\tLoss 105.4041 (777.0868)\t\n",
      "Epoch: [1][7/19]\tTime 107.211 (110.792)\tData 0.002 (0.033)\tLoss 32.0118 (670.6475)\t\n",
      "Epoch: [1][8/19]\tTime 67.074 (105.328)\tData 0.001 (0.029)\tLoss 8.2663 (587.8499)\t\n",
      "Epoch: [1][9/19]\tTime 80.430 (102.561)\tData 0.001 (0.026)\tLoss 10.2916 (523.6767)\t\n",
      "Epoch: [1][10/19]\tTime 92.955 (101.601)\tData 0.001 (0.023)\tLoss 23.1309 (473.6222)\t\n",
      "Epoch: [1][11/19]\tTime 78.477 (99.498)\tData 0.001 (0.021)\tLoss 97.6754 (439.4452)\t\n",
      "Epoch: [1][12/19]\tTime 78.870 (97.779)\tData 0.001 (0.020)\tLoss 18.3183 (404.3513)\t\n",
      "Epoch: [1][13/19]\tTime 106.320 (98.436)\tData 0.001 (0.018)\tLoss 65.3430 (378.2737)\t\n",
      "Epoch: [1][14/19]\tTime 70.736 (96.458)\tData 0.001 (0.017)\tLoss 26.6388 (353.1569)\t\n",
      "Epoch: [1][15/19]\tTime 69.934 (94.690)\tData 0.001 (0.016)\tLoss 13.4960 (330.5129)\t\n",
      "Epoch: [1][16/19]\tTime 90.971 (94.457)\tData 0.001 (0.015)\tLoss 41.0265 (312.4200)\t\n",
      "Epoch: [1][17/19]\tTime 162.102 (98.436)\tData 0.001 (0.014)\tLoss 81.3794 (298.8294)\t\n",
      "Epoch: [1][18/19]\tTime 66.246 (96.648)\tData 0.001 (0.013)\tLoss 7.3300 (282.6350)\t\n",
      "Epoch: [1][19/19]\tTime 53.777 (94.392)\tData 0.001 (0.013)\tLoss 9.9100 (275.2640)\t\n",
      "Training Summary Epoch: [1]\tTime taken (s): 1793\tAverage Loss 268.281\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:18<00:00, 11.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [1]\tAverage WER 196.154\tAverage CER 41.407\t\n",
      "Learning rate annealed to: 0.000273\n",
      "Found better validated model, saving to models/deepspeech_final.pth\n",
      "Shuffling batches...\n"
     ]
    }
   ],
   "source": [
    "%run deepspeech.pytorch/train.py --train-manifest data/train.csv --val-manifest data/test.csv --epochs 1 --batch-size 2 --labels-path=deepspeech.pytorch/labels.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
